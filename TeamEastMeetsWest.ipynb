{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import roc_curve, auc, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had a number of methods in play in order to track our model development and save time on failed runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ignore warning to present clean output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Added ability to debug with smaller datasets\n",
    "DEBUG = False\n",
    "if DEBUG:\n",
    "    TRAINING_DATA = 'trainingDataSubSet.txt'\n",
    "    TRAINING_TRUTH = 'trainingTruthSubSet.txt'\n",
    "    TEST_DATA = 'testDataSubSet.txt'\n",
    "else:\n",
    "    TRAINING_DATA = 'trainingData.txt'\n",
    "    TRAINING_TRUTH = 'trainingTruth.txt'\n",
    "    TEST_DATA = 'testData.txt'\n",
    "    \n",
    "def initLogging():\n",
    "    \n",
    "    # Initialize log confid\n",
    "    log_fn = './HW3_run_%s.log'%file_id\n",
    "                        \n",
    "    # create logger \n",
    "    global logger\n",
    "    logger = logging.getLogger('HW3')\n",
    "    \n",
    "    # reset handlers so as not to have to exit shell \n",
    "    # between two executions\n",
    "    logger.handlers = []\n",
    "    \n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    \n",
    "    # create file handler which logs even debug messages\n",
    "    fh = logging.FileHandler(log_fn)\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "    \n",
    "    # create console handler with a higher log level\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.ERROR)\n",
    "    \n",
    "    # create formatter and add it to the handlers\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s: %(message)s',\n",
    "                                  datefmt='%m/%d/%Y %I:%M:%S %p')\n",
    "    fh.setFormatter(formatter)\n",
    "    ch.setFormatter(formatter)\n",
    "    \n",
    "    # add the handlers to the logger\n",
    "    logger.addHandler(fh)\n",
    "    logger.addHandler(ch)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we do is try to get an overall idea of what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exploreData(X):\n",
    "    # Do initial analysis of the data\n",
    "    plt.hist(X.var(axis=0))\n",
    "    plt.xlabel('variance')\n",
    "    plt.ylabel('frequency')\n",
    "    plt.show()\n",
    "    plt.hist(X.mean(axis=0))\n",
    "    plt.xlabel('mean')\n",
    "    plt.ylabel('frequency')\n",
    "    plt.show()\n",
    "    \n",
    "    # This takes too long with all the rows, so we use a subset\n",
    "    # We see a similar range of values in all columns\n",
    "    sns.heatmap(X[0:20], xticklabels=20, yticklabels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to impute missing values using means, as it seemed to work fairly well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replaceMissingValues(X):\n",
    "    # Impute missing values, we can choose, mean, median or most frequent\n",
    "    # Choosing mean as a standard\n",
    "    strategy = 'mean'\n",
    "    imp = Imputer(missing_values='NaN', copy=False, strategy=strategy, axis=0)\n",
    "    imp.fit_transform(X)   \n",
    "    logger.info('Missing values replaced using %s' % strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also tried a couple of methods for feature reduction. The first was by variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduceFeaturesbyVariance(Xtrain, Xtest, threshold = 0.22):\n",
    "    # one way of removing low importance features is to \n",
    "    # remove features with low variability\n",
    "    # Looking at the variance histogram, we can \n",
    "    # choose 0.22 as a good cutoff\n",
    "    selector = VarianceThreshold(threshold = threshold)\n",
    "    selector.fit(Xtrain)\n",
    "    \n",
    "    # Print out the number of features retained\n",
    "    kept_features = selector.get_support(indices=True)\n",
    "    logger.info('Variance Threshold %0.2f: Keeping %d, out of %d features' \n",
    "                % (threshold, len(kept_features), Xtrain.shape[1]))\n",
    "\n",
    "    # Reduce dataset to only include selected features\n",
    "    Xtrain = selector.transform(Xtrain)\n",
    "    Xtest = selector.transform(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately we went with the random forests method for determining feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduceFeatureswithExtraTrees(Y, Xtrain, Xtest):\n",
    "    # Build a forest and compute the feature importances\n",
    "    forest = ExtraTreesClassifier(n_estimators=250,\n",
    "                                  random_state=0)\n",
    "    \n",
    "    forest.fit(Xtrain, Y)\n",
    "    importances = forest.feature_importances_\n",
    "    \n",
    "    # Compute the std. deviations\n",
    "    std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "                 axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    # Log top 10 features\n",
    "    logger.info(\"Feature ranking:\")\n",
    "    for f in range(10):\n",
    "        logger.info(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "    \n",
    "    # Plot the feature importances of the forest\n",
    "    plt.figure()\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.bar(range(Xtrain.shape[1]), importances[indices],\n",
    "           color=\"r\", yerr=std[indices], align=\"center\")\n",
    "    plt.xticks(range(Xtrain.shape[1]), indices, rotation=90)\n",
    "    plt.xlim([-1, Xtrain.shape[1]])\n",
    "    ax = plt.axes()\n",
    "    # Skip some of the feature labels to reduce crowding\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(15))\n",
    "    ax.set_xlabel('features') \n",
    "    ax.set_ylabel('importance')\n",
    "    plt.show()\n",
    "    \n",
    "    # select features based on importance weights.\n",
    "    # by default it uses mean importance as the threshold\n",
    "    selector = SelectFromModel(forest, prefit=True)\n",
    "        \n",
    "    # Print out the number of features retained\n",
    "    kept_features = selector.get_support(indices=True)\n",
    "    logger.info('ExtraTreeClassifier: Keeping %d, out of %d features' %\n",
    "                (len(kept_features), Xtrain.shape[1]))\n",
    "            \n",
    "    # Reduce dataset to only include selected features    \n",
    "    Xtrain = selector.transform(Xtrain)\n",
    "    Xtest = selector.transform(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also tried a couple of different ways of scoring our results, since when we first started we had ridiculously high AUC scores and felt we needed to use a second method to make sure it wasn't all a fluke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAUCByClass(model, X, Y, classes=[1, 2, 3, 4]):\n",
    "    \n",
    "    # Get the predictions\n",
    "    model_predict = model.predict_proba(X)\n",
    "\n",
    "    # Binarize the output\n",
    "    y_bin = label_binarize(Y, classes=classes)\n",
    "    \n",
    "    #Calculate AUC\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(4):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], model_predict[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    return(roc_auc)\n",
    "    \n",
    "def getF1ScoreByClass(model, X, Y, classes=[1, 2, 3, 4]):\n",
    "    \n",
    "    # Get the predictions\n",
    "    model_predict = model.predict_proba(X)\n",
    "    # Binarize the predictions\n",
    "    model_predict = (model_predict == model_predict.max(axis=1, keepdims=True)).astype(int)\n",
    "\n",
    "    # Binarize the output\n",
    "    y_bin = label_binarize(Y, classes=classes)\n",
    "    \n",
    "    # Calculate F1 scores\n",
    "    f1_scores = dict()\n",
    "    for i in range(4):\n",
    "        f1_scores[i] = f1_score(y_bin[:, i], model_predict[:, i])\n",
    "    \n",
    "    return(f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions and submission file creation (largely unchanged from the original code you provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createSubmission(model, Xtest, filename):\n",
    "    #Create submission\n",
    "    y_final_prob = model.predict_proba(Xtest)\n",
    "    y_final_label = model.predict(Xtest)\n",
    "    \n",
    "    sample = pd.DataFrame(np.hstack([y_final_prob.round(5),y_final_label.reshape(y_final_prob.shape[0],1)]))\n",
    "    sample.columns = [\"prob1\",\"prob2\",\"prob3\",\"prob4\",\"label\"]\n",
    "    sample.label = sample.label.astype(int)\n",
    "    \n",
    "    #Submit this file to dropbox\n",
    "    sample.to_csv(filename,sep=\"\\t\" ,index=False, header=None)\n",
    "    logger.info('Submission file created: %s' % filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started out with the method in the code provided at the beginning. Since there are so many parameters to tweak for each classifier, we ran a grid search to more efficiently identify the optimal parameters for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runRandomForestwithGridSearch(Y, Xtrain, Xtest):\n",
    "    \n",
    "    # Note time to run this setup \n",
    "    run_start = time.time()\n",
    "    \n",
    "    # Reduce feature based on importance\n",
    "    reduceFeatureswithExtraTrees(Y, Xtrain, Xtest)\n",
    "    \n",
    "    # Specify the parameters to tune\n",
    "    param_grid = {'estimator__n_estimators':[20, 30], \n",
    "                  'estimator__max_depth':[10, 20], \n",
    "                  'estimator__min_samples_split':[4, 6],\n",
    "                  'estimator__min_samples_leaf':[2, 4],\n",
    "                  'estimator__max_features': ['sqrt', 0.25]}\n",
    "    \n",
    "    model_to_set = OneVsRestClassifier(RandomForestClassifier(random_state=25, oob_score = True), -1)\n",
    "\n",
    "    gs_start = time.time()\n",
    "    model_tuning = GridSearchCV(model_to_set, \n",
    "                            param_grid = param_grid, \n",
    "                            scoring='f1_weighted',\n",
    "                            iid=False)\n",
    "    gs_end = time.time()\n",
    "    logger.info('Time to run grid search (RandomForest): %0.3fs'% (gs_end - gs_start))\n",
    "\n",
    "    # Fit the model\n",
    "    model_tuning.fit(Xtrain, Y)\n",
    "        \n",
    "    logger.info('Best score = %d' % model_tuning.best_score_)\n",
    "    logger.info('Best params = %s' % model_tuning.best_params_)\n",
    "    logger.info('AUC per class = %s' % \n",
    "                getAUCByClass(model_tuning, Xtrain, Y, classes=[1, 2, 3, 4]))\n",
    "    logger.info('F1 Score per class = %s' % \n",
    "                getF1ScoreByClass(model_tuning, Xtrain, Y, classes=[1, 2, 3, 4]))\n",
    "                \n",
    "    # Create results filename \n",
    "    result_fn = 'TeamEastMeetsWest-%s.csv'%file_id  \n",
    "    \n",
    "    createSubmission(model_tuning, Xtest, result_fn)\n",
    "    \n",
    "    # Note the end time\n",
    "    run_end = time.time()\n",
    "    logger.info('Time to run analysis(RandomForest): %0.3fs'% (run_end - run_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second method we tried was using SVM. We found that even with mostly default parameters, SVM gave us a much better-performing model (in terms of prediction accuracy) than the random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runSVM(Y, Xtrain, Xtest):\n",
    "    # Note time to run this setup\n",
    "    run_start = time.time()\n",
    "\n",
    "    # Normalize data since accuracy of SVM can severely degrade if it isn't\n",
    "    # Scale data to normal distribution (gaussian,  mean = 0, variance = 1)\n",
    "    scaler = StandardScaler().fit(Xtrain)\n",
    "    X_scaled = scaler.transform(Xtrain)\n",
    "    Xtest_scaled = scaler.transform(Xtest)\n",
    "\n",
    "    # Reduce feature based on importance\n",
    "    reduceFeatureswithExtraTrees(Y, X_scaled, Xtest_scaled)\n",
    "\n",
    "    # Guessing on these parameters because running with gridsearch took much too long, so I'm\n",
    "    #   using the results of smaller runs (largest size = first 5000 rows of training dataset)\n",
    "    clf = SVC(probability=True, cache_size=1000, C=0.1, kernel='sigmoid', gamma=0.1, class_weight='balanced')\n",
    "\n",
    "    # Fit the model\n",
    "    clf.fit(X_scaled, Y)\n",
    "\n",
    "    logger.info('AUC per class = %s' %\n",
    "                getAUCByClass(clf, X_scaled, Y, classes=[1, 2, 3, 4]))\n",
    "    logger.info('F1 Score per class = %s' %\n",
    "                getF1ScoreByClass(clf, Xtrain, Y, classes=[1, 2, 3, 4]))\n",
    "\n",
    "    # Create results filename\n",
    "    result_fn = 'TeamEastMeetsWest-%s.csv' % file_id\n",
    "\n",
    "    if not DEBUG:\n",
    "        # Predict for the test data and create submission\n",
    "        createSubmission(clf, Xtest_scaled, result_fn)\n",
    "\n",
    "    run_end = time.time()\n",
    "    logger.info('Time to run analysis(SVC): %0.3fs' % (run_end - run_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also used the grid search method with SVM, but it took so much longer than the previous grid searches (we estimate it would have taken ~18 hours to complete) that we ended up just running it on a few subsets of the data to get an idea of which parameters to focus on and some starting values, and running plain SVM on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runSVMwithGridSearch(Y, Xtrain, Xtest):\n",
    "    \n",
    "    # Note time to run this setup \n",
    "    run_start = time.time()\n",
    "       \n",
    "    # Normalize data since accuracy of SVM can severely degrade if it isn't\n",
    "    # Scale data to normal distribution (gaussian,  mean = 0, variance = 1)\n",
    "    scaler = StandardScaler().fit(Xtrain)\n",
    "    X_scaled = scaler.transform(Xtrain)\n",
    "    Xtest_scaled = scaler.transform(Xtest)\n",
    "    \n",
    "    # Reduce feature based on importance\n",
    "    reduceFeatureswithExtraTrees(Y, X_scaled, Xtest_scaled)\n",
    "    \n",
    "    param_grid = {\n",
    "        'kernel': ['poly', 'linear', 'sigmoid'],\n",
    "        'degree': [2, 4, 5],\n",
    "        'gamma': [.1, .25, .5],\n",
    "        'C': [.1, .25, .5]\n",
    "    }\n",
    "    \n",
    "    clf = SVC(probability=True, cache_size=1000)\n",
    "    \n",
    "    gs_start = time.time()\n",
    "    clf_tuned = GridSearchCV(clf, param_grid=param_grid)\n",
    "    gs_end = time.time()\n",
    "    \n",
    "    logger.info('Time to run grid search(SVC): %0.3fs'% (gs_end - gs_start))\n",
    "\n",
    "    # Fit the model\n",
    "    clf_tuned.fit(X_scaled, Y)\n",
    "\n",
    "    logger.info('Best score = %d' % clf_tuned.best_score_)\n",
    "    logger.info('Best params = %s' % clf_tuned.best_params_)\n",
    "    logger.info('AUC per class = %s' % \n",
    "                getAUCByClass(clf_tuned, X_scaled, Y, classes=[1, 2, 3, 4]))\n",
    "    logger.info('F1 Score per class = %s' % \n",
    "                getF1ScoreByClass(clf_tuned, Xtrain, Y, classes=[1, 2, 3, 4]))\n",
    "                \n",
    "    # Create results filename \n",
    "    result_fn = 'TeamEastMeetsWest-%s.csv'%file_id  \n",
    "    \n",
    "    # Predict for the test data and create submission\n",
    "    createSubmission(clf_tuned, Xtest_scaled, result_fn)\n",
    "    \n",
    "    run_end = time.time()\n",
    "    logger.info('Time to run analysis(SVC): %0.3fs'% (run_end - run_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we have defined the various functions we developed in our exploration. Here we show our path to our current model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_id = '%s'%datetime.now().strftime('%m-%d-%Y_%H%M')\n",
    "\n",
    "# initialize logging\n",
    "initLogging()\n",
    "\n",
    "logger.info(\"Starting run ...\")\n",
    "\n",
    "# Reading files\n",
    "read_start = time.time()\n",
    "\n",
    "# Read in the data\n",
    "X = pd.read_csv(TRAINING_DATA, sep='\\t', header=None)\n",
    "Y = pd.read_csv(TRAINING_TRUTH, sep='\\t', header=None)\n",
    "Xtest = pd.read_csv(TEST_DATA, sep=\"\\t\", header=None)\n",
    "\n",
    "read_end = time.time()\n",
    "\n",
    "# Print some timings\n",
    "logger.info('Time to load data: %0.3fs' % (read_end - read_start))\n",
    "\n",
    "# Log the size of data\n",
    "logger.info('X.shape: %s, Y.shape: %s, Xtest.shape: %s' %\n",
    "    (X.shape, Y.shape, Xtest.shape))\n",
    "\n",
    "# Flatten output labels array\n",
    "Y = np.array(Y).ravel()\n",
    "\n",
    "# Do some data exploration\n",
    "exploreData(X)\n",
    "\n",
    "# Replace missing values\n",
    "replaceMissingValues(X)\n",
    "\n",
    "# Run randomforest classifier with gridsearch\n",
    "#runRandomForestwithGridSearch(Y, X, Xtest)\n",
    "\n",
    "# Run SVM classifier with gridsearch\n",
    "# runSVMwithGridSearch(Y, X, Xtest)\n",
    "\n",
    "# Run SVM classifier without gridsearch\n",
    "runSVM(Y, X, Xtest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
